{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29c083e8",
   "metadata": {},
   "source": [
    "## 1. Inicializar Spark y preparar Spark SQL\n",
    "\n",
    "En esta sección:\n",
    "- Importamos las librerías necesarias.\n",
    "- Creamos una `SparkSession`.\n",
    "- Dejamos lista la sesión para trabajar con **DataFrames** y **Spark SQL** (consultas SQL sobre vistas temporales)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b18842b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Importar librerías y crear SparkSession orientada a Spark SQL\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "import os, sys\n",
    "\n",
    "# Aseguramos que Spark use el mismo intérprete de Python que este notebook\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "\n",
    "# Creamos/obtenemos una sesión de Spark\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"AnalisisDiscursoPresidencialSparkSQL\")\n",
    "    .config(\"spark.pyspark.python\", sys.executable)\n",
    "    .config(\"spark.pyspark.driver.python\", sys.executable)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"Versión de Spark:\", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2f2993",
   "metadata": {},
   "source": [
    "## 2. Cargar el archivo de texto como DataFrame y vista SQL\n",
    "\n",
    "En esta sección:\n",
    "- Definimos la ruta del archivo `05 - discurso.txt`.\n",
    "- Cargamos el archivo usando `spark.read.text` en un DataFrame.\n",
    "- Registramos una vista temporal para poder consultarlo con Spark SQL.\n",
    "- Vemos algunas líneas para verificar la lectura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec17617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Cargar el archivo de discurso como DataFrame y crear vista SQL\n",
    "\n",
    "# Ruta del archivo de texto.\n",
    "# Si el notebook está en la misma carpeta que el archivo (M8/S2),\n",
    "# esta ruta relativa debería funcionar directamente (en Colab suele ser /content/...).\n",
    "file_path = \"/content/05 - discurso.txt\"\n",
    "\n",
    "# Cargamos el archivo como un DataFrame de una sola columna llamada \"value\"\n",
    "lines_df = spark.read.text(file_path)\n",
    "\n",
    "# Renombramos la columna para que sea más descriptiva\n",
    "lines_df = lines_df.withColumnRenamed(\"value\", \"line\")\n",
    "\n",
    "# Registramos una vista temporal para usar Spark SQL\n",
    "lines_df.createOrReplaceTempView(\"discurso\")\n",
    "\n",
    "print(\"Número aproximado de líneas:\", lines_df.count())\n",
    "print(\"\\nPrimeras líneas del discurso:\\n\")\n",
    "for row in lines_df.limit(5).collect():\n",
    "    print(row[\"line\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293856e4",
   "metadata": {},
   "source": [
    "## 3. Limpieza de texto y definición de stopwords\n",
    "\n",
    "En esta sección:\n",
    "- Pasamos el texto a minúsculas.\n",
    "- Eliminamos signos de puntuación y caracteres no alfabéticos.\n",
    "- Separamos el texto en palabras.\n",
    "- Definimos una lista de **stopwords** (palabras sin mucho significado para el análisis: artículos, pronombres, etc.).\n",
    "- Generamos un DataFrame de palabras limpias y lo registramos como vista para usarlo desde **Spark SQL**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e541f9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Funciones de limpieza, lista de stopwords y generación de DataFrame de palabras\n",
    "\n",
    "import re\n",
    "\n",
    "# Lista de palabras vacías (stopwords) en español.\n",
    "# Se puede ampliar según necesidad.\n",
    "stopwords = {\n",
    "    \"a\", \"e\", \"i\", \"o\", \"u\",\n",
    "    \"el\", \"la\", \"los\", \"las\", \"un\", \"una\", \"unos\", \"unas\",\n",
    "    \"y\", \"o\", \"u\", \"de\", \"del\", \"al\", \"en\", \"con\", \"por\", \"para\",\n",
    "    \"como\", \"que\", \"se\", \"su\", \"sus\", \"es\", \"son\", \"ser\", \"fue\", \"fueron\",\n",
    "    \"este\", \"esta\", \"estos\", \"estas\", \"ese\", \"esa\", \"esos\", \"esas\",\n",
    "    \"yo\", \"tú\", \"tu\", \"usted\", \"ustedes\", \"nosotros\", \"nosotras\",\n",
    "    \"ellos\", \"ellas\",\n",
    "    \"mi\", \"mis\", \"nuestro\", \"nuestra\", \"nuestros\", \"nuestras\",\n",
    "    \"su\", \"sus\",\n",
    "    \"un\", \"una\", \"al\", \"del\",\n",
    "    \"pero\", \"también\", \"ya\", \"muy\", \"más\", \"menos\",\n",
    "    \"que\", \"porque\", \"cuando\", \"donde\",\n",
    "    \"hoy\", \"ayer\", \"mañana\",\n",
    "    \"es\", \"son\", \"era\", \"eran\", \"ser\", \"estar\", \"está\", \"están\"\n",
    "}\n",
    "\n",
    "def limpiar_y_dividir(linea):\n",
    "    \"\"\"Convierte la línea a minúsculas, elimina puntuación y la separa en palabras.\"\"\"\n",
    "    # Pasamos a minúsculas\n",
    "    linea = linea.lower()\n",
    "    # Reemplazamos cualquier caracter que no sea letra (incluye tildes y ñ) por un espacio\n",
    "    # Conservamos letras a-z, áéíóúñü\n",
    "    linea = re.sub(r\"[^a-záéíóúñü]+\", \" \", linea)\n",
    "    # Dividimos en palabras por espacios\n",
    "    palabras = linea.split()\n",
    "    return palabras\n",
    "\n",
    "# Probamos la función de limpieza con una línea de ejemplo\n",
    "ejemplo = \"¡Queridos compatriotas chilenos! Hoy me dirijo a ustedes...\"\n",
    "print(\"Línea original:\", ejemplo)\n",
    "print(\"Palabras limpias:\", limpiar_y_dividir(ejemplo))\n",
    "\n",
    "# Usamos la función en el DataFrame para obtener un DataFrame de palabras\n",
    "lines_con_palabras_df = lines_df.withColumn(\n",
    "    \"palabras\",\n",
    "    F.udf(limpiar_y_dividir, \"array<string>\")(F.col(\"line\"))\n",
    ")\n",
    "\n",
    "# Explosionamos el array de palabras para tener una fila por palabra\n",
    "palabras_df = lines_con_palabras_df.select(F.explode(\"palabras\").alias(\"word\"))\n",
    "\n",
    "# Registramos la vista de palabras para consultar con Spark SQL\n",
    "palabras_df.createOrReplaceTempView(\"palabras\")\n",
    "\n",
    "palabras_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68623f6",
   "metadata": {},
   "source": [
    "## 4. Pipeline con Spark SQL: contar palabras\n",
    "\n",
    "Ahora realizamos el conteo de palabras usando **consultas Spark SQL** sobre la vista `palabras`:\n",
    "\n",
    "1. Filtramos **stopwords** directamente en la consulta SQL.\n",
    "2. Agrupamos por palabra (`GROUP BY`).\n",
    "3. Contamos cuántas veces aparece cada palabra (`COUNT(*)`).\n",
    "4. Ordenamos por frecuencia de mayor a menor (`ORDER BY`).\n",
    "5. Obtenemos las **10 palabras más frecuentes** usando `LIMIT 10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddafda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Construir el pipeline de conteo de palabras usando Spark SQL\n",
    "\n",
    "# Generamos una lista de stopwords en formato SQL ('palabra1','palabra2',...)\n",
    "stopwords_list = sorted(list(stopwords))\n",
    "stopwords_sql_list = \",\".join(f\"'{w}'\" for w in stopwords_list)\n",
    "\n",
    "consulta_top_10 = f\"\"\"\n",
    "SELECT\n",
    "    word,\n",
    "    COUNT(*) AS frecuencia\n",
    "FROM palabras\n",
    "WHERE word NOT IN ({stopwords_sql_list})\n",
    "GROUP BY word\n",
    "ORDER BY frecuencia DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "top_10_palabras_df = spark.sql(consulta_top_10)\n",
    "\n",
    "top_10_palabras_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f3418a",
   "metadata": {},
   "source": [
    "## 5. Mostrar los 10 principales términos formateados\n",
    "\n",
    "En esta sección simplemente mostramos los resultados de forma más amigable:\n",
    "- Palabra.\n",
    "- Cantidad de apariciones.\n",
    "\n",
    "Usaremos `collect()` para traer los resultados a Python y formatearlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85f419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Imprimir los 10 términos más frecuentes de forma legible\n",
    "\n",
    "print(\"Top 10 palabras más frecuentes (sin stopwords):\\n\")\n",
    "for row in top_10_palabras_df.collect():\n",
    "    palabra = row[\"word\"]\n",
    "    frecuencia = row[\"frecuencia\"]\n",
    "    print(f\"{palabra:15s} -> {frecuencia}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d29e07e",
   "metadata": {},
   "source": [
    "## 6. Cierre de la sesión de Spark\n",
    "\n",
    "Al finalizar el análisis, es buena práctica detener la sesión de Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3749c194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Detener la sesión de Spark\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
