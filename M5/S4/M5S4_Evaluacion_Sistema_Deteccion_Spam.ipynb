{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNpT3UKrcqXu"
      },
      "source": [
        "---\n",
        "# Evaluacion Sistema Spam\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0BaEpFk7cqXx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vN_zaSO6cqXz"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('datos-spam.txt', sep='|', skiprows=2, header=None, names=['A','Real','Predicho','D'], usecols=['Real','Predicho'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "CUTmCYGkcqX2",
        "outputId": "42d62bc6-3c06-4882-96ce-2fc0221b3e89"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Real  Predicho\n",
              "0     0         1\n",
              "1     0         1\n",
              "2     1         1\n",
              "3     1         1\n",
              "4     0         0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b20c7b1a-f86d-4234-8930-2ebe1396bc1f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Real</th>\n",
              "      <th>Predicho</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b20c7b1a-f86d-4234-8930-2ebe1396bc1f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b20c7b1a-f86d-4234-8930-2ebe1396bc1f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b20c7b1a-f86d-4234-8930-2ebe1396bc1f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 30,\n  \"fields\": [\n    {\n      \"column\": \"Real\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Predicho\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WYDawDyVcqX8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oj4DQyhgcqX9"
      },
      "outputs": [],
      "source": [
        "y_real = df['Real']\n",
        "y_pred = df['Predicho']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTbiOw0TcqX-",
        "outputId": "6a63f09f-94ba-46d4-a8cf-061a4fce2052"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Métricas de evaluación:\n",
            "Accuracy: 0.8\n",
            "Precision: 0.7\n",
            "Recall: 1.0\n",
            "F1-score: 0.8235294117647058\n",
            "\n",
            "Matriz de Confusión:\n",
            "[[10  6]\n",
            " [ 0 14]]\n",
            "\n",
            "Tasas de Falsos Positivos y Negativos:\n",
            "Tasa de Falsos Positivos (FPR): 0.375\n",
            "Tasa de Falsos Negativos (FNR): 0.0\n"
          ]
        }
      ],
      "source": [
        "# Calculamos las métricas de evaluación\n",
        "accuracy = accuracy_score(y_real, y_pred)\n",
        "precision = precision_score(y_real, y_pred)\n",
        "recall = recall_score(y_real, y_pred)\n",
        "f1 = f1_score(y_real, y_pred)\n",
        "conf_matrix = confusion_matrix(y_real, y_pred)\n",
        "\n",
        "# Calculamos las tasas de falsos positivos y negativos\n",
        "FP = conf_matrix[0,1]\n",
        "FN = conf_matrix[1,0]\n",
        "TN = conf_matrix[0,0]\n",
        "TP = conf_matrix[1,1]\n",
        "FPR = FP / (FP + TN)\n",
        "FNR = FN / (FN + TP)\n",
        "\n",
        "# Mostramos los resultados\n",
        "print(\"Métricas de evaluación:\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)\n",
        "print(\"\\nMatriz de Confusión:\")\n",
        "print(conf_matrix)\n",
        "print(\"\\nTasas de Falsos Positivos y Negativos:\")\n",
        "print(\"Tasa de Falsos Positivos (FPR):\", FPR)\n",
        "print(\"Tasa de Falsos Negativos (FNR):\", FNR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6E0AO8JcqX_"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "### **Análisis de las métricas**\n",
        "\n",
        "- **Accuracy (Exactitud) = 0.8:**  \n",
        "  El 80% de las clasificaciones realizadas por el modelo son correctas en general.\n",
        "\n",
        "- **Precision = 0.7:**  \n",
        "  Cuando el modelo predice que algo es spam, tiene razón el 70% de las veces. Esto significa que hay algunos casos donde clasifica incorrectamente mensajes legítimos como spam (falsos positivos).\n",
        "\n",
        "- **Recall = 1.0:**  \n",
        "  El modelo es capaz de identificar absolutamente todos los mensajes que son realmente spam. No se le escapa ningún spam, lo que significa que no hay falsos negativos.\n",
        "\n",
        "- **F1-score = 0.82:**  \n",
        "  Este valor, que considera tanto la precisión como la capacidad de detección (recall), indica que el modelo tiene un rendimiento general bastante sólido y equilibrado.\n",
        "\n",
        "---\n",
        "\n",
        "### **Matriz de Confusión**\n",
        "```\n",
        "[[10  6]\n",
        " [ 0 14]]\n",
        "```\n",
        "- **10:** No spam correctamente clasificados.\n",
        "- **6:** No spam clasificados erróneamente como spam (falsos positivos).\n",
        "- **0:** Spam clasificados erróneamente como no spam (falsos negativos).\n",
        "- **14:** Spam correctamente clasificados.\n",
        "\n",
        "---\n",
        "\n",
        "### **Tasas de error**\n",
        "- **FPR (Falsos Positivos) = 0.375:**  \n",
        "  El 37.5% de los mensajes que no son spam fueron clasificados incorrectamente como spam.\n",
        "\n",
        "- **FNR (Falsos Negativos) = 0.0:**  \n",
        "  No hubo mensajes de spam que el modelo dejara pasar como no spam.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusiones**\n",
        "\n",
        "- **Priorización de la seguridad sobre la conveniencia:** El modelo está fuertemente inclinado a asegurar que ningún spam llegue a la bandeja de entrada, lo que es excelente para la seguridad, pero lo hace a costa de una mayor tasa de \"falsas alarmas\" en correos legítimos.\n",
        "\n",
        "- **Impacto en la experiencia del usuario:** Una alta tasa de falsos positivos (FPR del 37.5%) puede resultar en una mala experiencia para el usuario, ya que mensajes importantes podrían ser clasificados como spam, obligando a los usuarios a revisar constantemente su carpeta de spam.\n",
        "\n",
        "- **Necesidad de ajuste:** Dependiendo del caso de uso, si la molestia de los falsos positivos supera el beneficio de un recall perfecto, sería fundamental ajustar el umbral de decisión del clasificador o explorar modelos que equilibren mejor la precisión y el recall para reducir los falsos positivos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3vXLSGqcqYA"
      },
      "source": [
        "---\n",
        "\n",
        "### 1. `y_proba_log = logreg.predict_proba(X_test_scaled)[:, 1]`\n",
        "- **¿Qué es?**  \n",
        "  Calcula la probabilidad de que cada instancia de prueba pertenezca a la clase positiva (por ejemplo, \"sí compra\").\n",
        "- **¿Para qué sirve?**  \n",
        "  Se usa para métricas como ROC AUC y para tomar decisiones basadas en probabilidades.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. `accuracy_score(y_test, y_pred)`\n",
        "- **Exactitud (Accuracy):**  \n",
        "  Es el porcentaje de predicciones correctas sobre el total de casos.\n",
        "- **¿Para qué sirve?**  \n",
        "  Mide qué tan bien el modelo clasifica en general, pero puede ser engañosa si las clases están desbalanceadas.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. `confusion_matrix(y_test, y_pred)`\n",
        "- **Matriz de confusión:**  \n",
        "  Es una tabla que muestra cuántos casos fueron clasificados correctamente o incorrectamente en cada clase.\n",
        "- **¿Para qué sirve?**  \n",
        "  Permite ver los errores específicos del modelo: falsos positivos, falsos negativos, verdaderos positivos y verdaderos negativos.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. `precision_score(y_test, y_pred)`\n",
        "- **Precisión (Precision):**  \n",
        "  Es la proporción de verdaderos positivos sobre el total de predicciones positivas.\n",
        "- **¿Para qué sirve?**  \n",
        "  Indica cuántas de las predicciones positivas realmente lo son. Es útil cuando el costo de un falso positivo es alto.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. `recall_score(y_test, y_pred)`\n",
        "- **Recall (Sensibilidad):**  \n",
        "  Es la proporción de verdaderos positivos sobre el total de casos positivos reales.\n",
        "- **¿Para qué sirve?**  \n",
        "  Mide la capacidad del modelo para encontrar todos los casos positivos. Es útil cuando el costo de un falso negativo es alto.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. `roc_auc_score(y_test, y_proba_log)`\n",
        "- **ROC AUC:**  \n",
        "  Es el área bajo la curva ROC, que mide la capacidad del modelo para distinguir entre clases.\n",
        "- **¿Para qué sirve?**  \n",
        "  Un valor cercano a 1 indica que el modelo separa bien las clases; 0.5 indica que no es mejor que el azar.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}