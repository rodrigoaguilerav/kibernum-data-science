{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Evaluacion Sistema Spam\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datos-spam.txt', sep='|', skiprows=2, header=None, names=['A','Real','Predicho','D'], usecols=['Real','Predicho'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Real",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Predicho",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "ea828ca8-eed9-46cf-9310-dd831e373f1f",
       "rows": [
        [
         "0",
         "0",
         "1"
        ],
        [
         "1",
         "0",
         "1"
        ],
        [
         "2",
         "1",
         "1"
        ],
        [
         "3",
         "1",
         "1"
        ],
        [
         "4",
         "0",
         "0"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Real</th>\n",
       "      <th>Predicho</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Real  Predicho\n",
       "0     0         1\n",
       "1     0         1\n",
       "2     1         1\n",
       "3     1         1\n",
       "4     0         0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_real = df['Real']\n",
    "y_pred = df['Predicho']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas de evaluación:\n",
      "Accuracy: 0.8\n",
      "Precision: 0.7\n",
      "Recall: 1.0\n",
      "F1-score: 0.8235294117647058\n",
      "\n",
      "Matriz de Confusión:\n",
      "[[10  6]\n",
      " [ 0 14]]\n",
      "\n",
      "Tasas de Falsos Positivos y Negativos:\n",
      "Tasa de Falsos Positivos (FPR): 0.375\n",
      "Tasa de Falsos Negativos (FNR): 0.0\n"
     ]
    }
   ],
   "source": [
    "# Calculamos las métricas de evaluación\n",
    "accuracy = accuracy_score(y_real, y_pred)\n",
    "precision = precision_score(y_real, y_pred)\n",
    "recall = recall_score(y_real, y_pred)\n",
    "f1 = f1_score(y_real, y_pred)\n",
    "conf_matrix = confusion_matrix(y_real, y_pred)\n",
    "\n",
    "# Calculamos las tasas de falsos positivos y negativos\n",
    "FP = conf_matrix[0,1]\n",
    "FN = conf_matrix[1,0]\n",
    "TN = conf_matrix[0,0]\n",
    "TP = conf_matrix[1,1]\n",
    "FPR = FP / (FP + TN)\n",
    "FNR = FN / (FN + TP)\n",
    "\n",
    "# Mostramos los resultados\n",
    "print(\"Métricas de evaluación:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "print(\"\\nMatriz de Confusión:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nTasas de Falsos Positivos y Negativos:\")\n",
    "print(\"Tasa de Falsos Positivos (FPR):\", FPR)\n",
    "print(\"Tasa de Falsos Negativos (FNR):\", FNR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **Análisis de las métricas**\n",
    "\n",
    "- **Accuracy (Exactitud) = 0.8:**  \n",
    "  El modelo clasifica correctamente el 80% de los casos.\n",
    "\n",
    "- **Precision = 0.7:**  \n",
    "  De todos los casos que el modelo predijo como spam, solo el 70% realmente lo eran. Hay falsos positivos.\n",
    "\n",
    "- **Recall = 1.0:**  \n",
    "  El modelo detectó todos los casos reales de spam (no dejó pasar ninguno). No hay falsos negativos.\n",
    "\n",
    "- **F1-score = 0.82:**  \n",
    "  Es una métrica balanceada entre precisión y recall. Indica buen desempeño general.\n",
    "\n",
    "---\n",
    "\n",
    "### **Matriz de Confusión**\n",
    "```\n",
    "[[10  6]\n",
    " [ 0 14]]\n",
    "```\n",
    "- **10:** No spam correctamente clasificados.\n",
    "- **6:** No spam clasificados erróneamente como spam (falsos positivos).\n",
    "- **0:** Spam clasificados erróneamente como no spam (falsos negativos).\n",
    "- **14:** Spam correctamente clasificados.\n",
    "\n",
    "---\n",
    "\n",
    "### **Tasas de error**\n",
    "- **FPR (Falsos Positivos) = 0.375:**  \n",
    "  El 37.5% de los mensajes que no son spam fueron clasificados incorrectamente como spam.\n",
    "\n",
    "- **FNR (Falsos Negativos) = 0.0:**  \n",
    "  No hubo mensajes de spam que el modelo dejara pasar como no spam.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusiones**\n",
    "\n",
    "- El modelo **no deja pasar ningún spam** (recall perfecto), lo cual es bueno si queremos evitar que el spam llegue al usuario.\n",
    "- Sin embargo, **clasifica erróneamente muchos mensajes legítimos como spam** (precisión y FPR bajos), lo que puede molestar a los usuarios.\n",
    "- El modelo es **muy estricto**: prefiere marcar de más antes que dejar pasar spam.\n",
    "- **Recomendación:**  \n",
    "  Si el costo de perder mensajes legítimos es alto, se debe ajustar el modelo para reducir falsos positivos (mejorar precisión). Si el costo de dejar pasar spam es alto, este modelo es adecuado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1. `y_proba_log = logreg.predict_proba(X_test_scaled)[:, 1]`\n",
    "- **¿Qué es?**  \n",
    "  Calcula la probabilidad de que cada instancia de prueba pertenezca a la clase positiva (por ejemplo, \"sí compra\").\n",
    "- **¿Para qué sirve?**  \n",
    "  Se usa para métricas como ROC AUC y para tomar decisiones basadas en probabilidades.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. `accuracy_score(y_test, y_pred)`\n",
    "- **Exactitud (Accuracy):**  \n",
    "  Es el porcentaje de predicciones correctas sobre el total de casos.\n",
    "- **¿Para qué sirve?**  \n",
    "  Mide qué tan bien el modelo clasifica en general, pero puede ser engañosa si las clases están desbalanceadas.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. `confusion_matrix(y_test, y_pred)`\n",
    "- **Matriz de confusión:**  \n",
    "  Es una tabla que muestra cuántos casos fueron clasificados correctamente o incorrectamente en cada clase.\n",
    "- **¿Para qué sirve?**  \n",
    "  Permite ver los errores específicos del modelo: falsos positivos, falsos negativos, verdaderos positivos y verdaderos negativos.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. `precision_score(y_test, y_pred)`\n",
    "- **Precisión (Precision):**  \n",
    "  Es la proporción de verdaderos positivos sobre el total de predicciones positivas.\n",
    "- **¿Para qué sirve?**  \n",
    "  Indica cuántas de las predicciones positivas realmente lo son. Es útil cuando el costo de un falso positivo es alto.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. `recall_score(y_test, y_pred)`\n",
    "- **Recall (Sensibilidad):**  \n",
    "  Es la proporción de verdaderos positivos sobre el total de casos positivos reales.\n",
    "- **¿Para qué sirve?**  \n",
    "  Mide la capacidad del modelo para encontrar todos los casos positivos. Es útil cuando el costo de un falso negativo es alto.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. `roc_auc_score(y_test, y_proba_log)`\n",
    "- **ROC AUC:**  \n",
    "  Es el área bajo la curva ROC, que mide la capacidad del modelo para distinguir entre clases.\n",
    "- **¿Para qué sirve?**  \n",
    "  Un valor cercano a 1 indica que el modelo separa bien las clases; 0.5 indica que no es mejor que el azar.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
